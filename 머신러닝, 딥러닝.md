## 김성훈 교수님의 모두를 위한 머신러닝/딥러닝 강의 시즌 1 – 딥러닝의 기본 총정리 (https://hunkim.github.io/ml/)
### 1. 머신러닝의 개념과 용어
#### [Supervised / Unsupervised Learning]
- Supervised Learning : labeled data를 가지고 학습
- UnSupervised Learning : un-labeled data를 가지고 학습
#### [Types of supervised learning]
- regression : final exam score based on time spent (0 ~ 100)
- binary classification : pass / non-pass
- multi-label classification : Letter grade (A, B, C, D and F)

#### [Regression]
|x|y|
|------|---|
|10|90|
|9|80|
|3|50|
|2|30|

↓

(training) 

↓

Regression

- 소요된 시간을 기준으로 한 시험점수 예측

#### [Binary classification]
|x|y|
|------|---|
|10|P|
|9|P|
|3|F|
|2|F|

↓

(training) 

↓

Binary classification

- 소요된 시간을 기준으로 한 합격 / 불합격

#### [Multi-label Classification]
|x|y|
|------|---|
|10|A|
|9|B|
|3|D|
|2|F|

↓

(training) 

↓

Multi-class classification

- 소요된 시간을 기준으로 A, B, C, D, F 등 등급으로 평가

### 2. Linear Regression의 개념 
#### [Regression (data)]

![image](https://user-images.githubusercontent.com/57993534/126035167-7232c721-3d8f-44a1-a337-6bc87215c092.png)

#### [(Linear) Hypothesis]

![image](https://user-images.githubusercontent.com/57993534/126035170-d73c28fa-aee8-40d1-b40c-594023d41c0f.png)

- W와 b에 따라 선이 나타남.

![image](https://user-images.githubusercontent.com/57993534/126035173-12776cd2-00ca-4d5d-bf7f-9a82fd4cf998.png)

- 실제 데이터와의 거리를 계산하여 좋은 가설인지를 판단 (cost function)

#### [Cost function]
- H(x) - y = 가설 값 - 실제 값
- ![image](https://user-images.githubusercontent.com/57993534/126036476-263f8b0f-4e2d-4b09-a610-8fc86ecd10ea.png) : 차이가 음수 / 양수 상관없음
- ![image](https://user-images.githubusercontent.com/57993534/126036504-dd1b9b82-3300-436e-8706-2ebccc79ee91.png)
- ![image](https://user-images.githubusercontent.com/57993534/126036537-49365e16-d8d9-4b9c-8fd2-d527d1676d78.png) : m은 데이터 개수
- ![image](https://user-images.githubusercontent.com/57993534/126036561-976eacf7-e7f6-4c61-bbab-696b1c84cf99.png) : 이 값을 가장 작게 하는 W, b 값을 찾아야 함 -> 이것을 찾는 것이 Linear Regression의 학습

### 3. Linear Regression cost 함수 최소화
#### [Simplified hypothesis]
- H(x) = Wx + b
- ![image](https://user-images.githubusercontent.com/57993534/126036650-b81b3c8b-3b6a-43e4-8135-94fe42853c5e.png)
- H(x) = Wx : b = 0을 대입하여 단순화시킨다. 
- ![image](https://user-images.githubusercontent.com/57993534/126036673-387e3f90-79c4-4060-961c-c4d550c63512.png)

![image](https://user-images.githubusercontent.com/57993534/126036677-a6c40254-e011-499b-8731-a3898da2e6c1.png)

- 아래의 값들을 이용하면 위 그래프처럼 표현된다.
    + W = 0일 때 cost(W) = 4.67 
    + W = 1일 때 cost(W) = 0
    + W = 2일 때 cost(W) = 4.67
- 결국, 목표는 cost 값이 최솟값이 되는 지점을 찾는 것

#### [Gradient descent algorithm]
- 경사를 따라 내려가는 알고리즘
- Minimize cost function : cost(W, b) - 최소의 W, b를 찾는 알고리즘
- 많은 minimize problems에 사용

#### [Gradient descent algorithm 작동방법]
1. 0.0(혹은 다른 값)에서 시작하여 W와 b를 조금씩 변경하여 cost(W, b)를 감소   
2. 파라미터를 변경할 때마다 cost(W, b)를 가장 많이 감소시키는 gradient를 선택   
3. 최저점으로 수렴할 때까지 반복   
(시작할 때 최종 최솟값을 설정할 수 있음)

#### [Gradient descent algorithm 계산 방식]
![image](https://user-images.githubusercontent.com/57993534/126036780-52719e22-e699-4dcd-8d37-5b1017165815.png) -> ![image](https://user-images.githubusercontent.com/57993534/126036788-a92dba15-92e2-4ec1-98e2-3d39cbad323d.png)   
![image](https://user-images.githubusercontent.com/57993534/126036837-6f49a20e-e060-4c57-9a47-83f8918ece2a.png)   
![image](https://user-images.githubusercontent.com/57993534/126036916-586e262f-5651-4e92-85ee-9439e629006c.png)   
![image](https://user-images.githubusercontent.com/57993534/126036955-4773d474-984a-4298-a405-cd4c69b4d278.png)   
![image](https://user-images.githubusercontent.com/57993534/126036991-79d1140d-cba4-4b93-baec-af6c26c14871.png)

- 이 식을 기계적으로 적용하면 cost function을 최소화하는 W를 구할 수 있고, 그것이 바로 Linear regression의 핵심인 학습 과정을 통해 모델을 만드는 과정이다.

### 4. 여러 개의 입력(feature)의 Linear Regression
#### [3개의 input(x1, x2, x3)을 사용하는 Regression]
- 일반적인 가설과 cost 함수 
H(x) = Wx + b   →   ![image](https://user-images.githubusercontent.com/57993534/126037063-56bb5c66-800c-423f-85fc-def3b968ab91.png)   
![image](https://user-images.githubusercontent.com/57993534/126037081-0e194974-3c6d-4ba3-9ac8-0332aba42646.png)    
: 변수가 많아지면 같은 방식으로 늘려줌. 하지만 이 방식을 사용하면 번거로워지므로 Matrix(행렬)를 사용함.

- Matrix(행렬)의 곱을 이용한 가설   
![image](https://user-images.githubusercontent.com/57993534/126037230-20e786f9-b5b3-42a9-abfe-597eb887255f.png)    
: H(X) = XW로 표현할 수 있음. (여기서 X와 W는 모두 행렬)

![image](https://user-images.githubusercontent.com/57993534/126037442-b906663c-a110-4a5d-a2ff-cfbebe5757d4.png)   
: 각각의 인스턴스를 계산할 필요 없이 전체를 긴 행렬에 넣어 W를 곱하면 됨. (W 행렬의 크기는 변수의 개수가 m개이고 라벨의 개수가 n개이면 [m, n])   
: Matrix를 사용하면 tensorflow의 shape의 instance의 수를 자동으로 채워줌. 즉, w = variable shape를 몰라도 자동으로 shape를 선정해줌.   


### 5. Logistic(Regression) Classification 
#### [Binary Classification]
- 소요된 시간을 기준으로 두 개 중 하나의 결과값을 가짐. (ex> P/F, 0/1)

#### [공부한 시간을 기준으로 Pass(1) or Fail(0)을 예측]
- Linear Regression을 통해 실제 데이터와 가장 근접한 선형적 모델을 만들고, 그 중간 지점을 찾아 그 지점보다 작으면 Fail(0), 크면 Pass(1)로 예측

![image](https://user-images.githubusercontent.com/57993534/126037479-bd41d86e-53d6-400c-a764-a96502e1dafe.png)    
: 이렇게 하면 training set의 값이 커지면 중간 지점이 달라지고, 그렇게 되면 이전에는 Pass(1)였던 시간도 Fail(0)로 예측될 수 있음.

#### [Logistic Classification의 가설(Hypothesis) 정의]
- Logistic Hypothesis   
: H(X) = WX를 z라고 하면 z 값에 상관없이 0에서 1 사이의 값이 나올 수 있도록 해주는 함수

![image](https://user-images.githubusercontent.com/57993534/126037540-d0880105-6262-47df-b8e2-3d2c9a8b96cd.png)

#### [Logistic Regression의 cost 함수]
: Linear Regression과 동일한 cost 함수를 사용하게 되면 cost 함수의 모양이 매끄럽지 못하므로 매끄럽게 바꿔주어야 함.    
![image](https://user-images.githubusercontent.com/57993534/126037590-38e50169-b0d7-4bfb-a09f-97895c369a9f.png)    
![image](https://user-images.githubusercontent.com/57993534/126037596-7342ce6b-af13-44cb-a688-af5a2f944a4c.png)    

#### [cost 함수의 최소화(Gradient descent algorithm)]
![image](https://user-images.githubusercontent.com/57993534/126037613-8a58a607-572c-4811-85c9-4406908311c5.png)    

### 6. Softmax Regression(Multinomial Logistic Regression) 
#### [Multinomial Classification]
![image](https://user-images.githubusercontent.com/57993534/126037630-6eaddb19-c8bc-4c3e-8290-5bcb0c1f4c0f.png)    
: Label의 종류가 세 개(A, B, C)인 Regression    
: 3개의 각각 다른 Binary Classification으로도 구현할 수 있음. (각각 독립된 Classifier들을 가지고 구현)    

#### [Classifier의 구현]
- Binary Classification

![image](https://user-images.githubusercontent.com/57993534/126037654-96a2586b-6b50-4da9-9aff-ff039866c98e.png)      
: 각각 독립된 Classifier들을 가지고 있어야 함. 이러한 방식을 이용하면 복잡하므로 Multinomial Classification을 이용함.

#### [Multinomial Classification]
![image](https://user-images.githubusercontent.com/57993534/126037659-4838e672-81ff-4541-9596-3a100b29d4f4.png)        
: 3번의 독립된 형태의 벡터를 계산하는 것이 아니라 Matrix multiplication을 이용하여 편리하게 계산

#### [Softmax 함수]
![image](https://user-images.githubusercontent.com/57993534/126037670-b0c89416-d934-4123-a2b8-40ba547b03a2.png)       
: softmax 함수를 이용하여 각각의 scores가 0에서 1 사이의 값을 가지고 합이 1인 확률(probabilities)로 변환

#### [One-hot encoding]
![image](https://user-images.githubusercontent.com/57993534/126037675-0af820af-a243-41cd-8f39-90d9f9e96147.png)       
: 제일 큰 값을 1로 만들고 나머지는 0으로 만든다.

#### [Cross-entropy Cost function]
- 하나의 training set에 대한 Cross-entropy cost function     
![image](https://user-images.githubusercontent.com/57993534/126037786-9d78c099-3bac-425e-99cb-c5ae3c687c85.png)      
![image](https://user-images.githubusercontent.com/57993534/126037744-a9b1d1c3-2131-4049-a1c8-877223dab8b2.png)      

- 여러 개의 training set에 대한 Cross-entropy cost function       
![image](https://user-images.githubusercontent.com/57993534/126037752-3e03aea0-57f0-4c3b-982c-6c720b000f42.png)      

#### [Cost function의 최소화(Gradient descent)]
![image](https://user-images.githubusercontent.com/57993534/126037790-a7e6db94-160d-4fea-8e6c-3e3483cec4a7.png)

### 7. ML의 실용과 몇 가지 팁
#### [Learning rate]
1. Large learning rate : overshooting     
![image](https://user-images.githubusercontent.com/57993534/126037796-1cbea54d-55d3-433c-ba95-bdfb88ca4b70.png)     
: learning rate가 너무 크면 경사면을 타고 튕겨 올라가다가 결국에는 밥그릇을 탈출하는 overshooting 현상이 발생함. 

2. Small learning rate     
![image](https://user-images.githubusercontent.com/57993534/126037802-242f53bb-4667-4c40-b42f-d063507c0fd6.png)      
: learning rate가 너무 작은 경우에는 한참을 내려갔음에도 불구하고 최저점까지 도달하지 못하는 현상이 발생함.    
- learning rate를 너무 크지도, 작지도 않게 조절하는 것이 중요    
- 다양한 learning rate를 사용하여 여러 번에 걸쳐 실행하는 것이 최선   

#### [Data(X) preprocessing for gradient descent]
- gradient descent algorithm을 적용하기 전에 preprocessing 작업으로 데이터의 범위를 제한할 수 있음.

#### [Feature Scaling]
1. Normalization(Re-Scaling)       
 : 전체 구간을 0 ~ 100으로 설정하여 데이터를 관찰하는 방법으로 특정 데이터의 위치를 확인할 수 있도록 함     
2. Standardization     
 : 평균까지의 거리로 2개 이상의 대상이 단위가 다를 때 대상 데이터를 같은 기준으로 볼 수 있도록 함     

#### [Overfitting]
- 모델이 training dataset에 잘 맞추기 위해 과도하게 복잡하여 실제로 사용할 때 맞지 않는 상황이 발생
- 해결책     
    1. training data 수 늘리기     
    2. 입력으로 들어오는 변수의 개수 줄이기    
    3. Regularization 사용      

#### [Regularization]
- W(weight)가 너무 큰 값들을 갖지 않도록 하는 것
- 모델의 복잡도를 낮추는 방법     

![image](https://user-images.githubusercontent.com/57993534/126037829-d447ac99-596f-4c05-b2c3-dcd8f945d545.png)

- cost 함수가 틀렸을 때 높은 비용이 발생할 수 있도록 penalty를 부과하는 것처럼 W에 대한 값이 크면 penalty 부여

#### [Training, Validation and test sets]

![image](https://user-images.githubusercontent.com/57993534/126037836-34ab5bdf-0a99-43cc-b063-545e6eeda495.png)     
: 위 그림처럼 학습을 시키기 위한 training set, 학습 결과를 확인하기 위한 test set이 필요     
: 여러 번에 걸쳐 학습(train)하고, 검증(test)하는 작업을 반복    

### 8. 딥러닝의 기본 개념과 문제, 그리고 해결
#### [Logistic regression Units]
![image](https://user-images.githubusercontent.com/57993534/126037844-7390c67c-89e1-4618-be86-19337ea21632.png)       
: Logistic Regression을 여러 번 적용하면 오른쪽과 같은 형태로 구성할 수 있음

#### [Backpropagation]   
![image](https://user-images.githubusercontent.com/57993534/126037851-e82bc9ad-d645-45c4-b3ce-d3352d21b713.png)      
- Output layer부터 Input layer까지 반대로 에러를 보정하는 기술
- layer가 많으면 뒤에서부터 멀리 떨어진 layer의 W와 b를 변경할 수 없는 엄청난 문제점 발생

#### [Breakthrough]
- 여러 개의 layer가 있어도 초깃값을 잘 선택하면 학습 가능
- 신경망을 잘 구성하면 복잡한 문제를 효율적으로 풀 수 있음

### 9. Neural Network 1: XOR 문제와 학습방법, Backpropagation (1986 breakthrough) 
#### [XOR using NN]
![image](https://user-images.githubusercontent.com/57993534/126037868-8bec614b-cc0f-45f6-91d8-4b559fe003b3.png)

- 3개의 logistic regression의 과정에서 마지막에는 sigmoid 과정이 있음
- sigmoid 과정으로 hypothesis의 결과는 항상 0과 1 사이의 값으로 조정됨
- 앞 두 개의 결과를 마지막 logistic regression의 입력으로 사용한다.

#### [Forward propagation]
![image](https://user-images.githubusercontent.com/57993534/126037873-ac7a1ebe-6313-4d55-8f30-5e7d3dc67a83.png)

: 이전의 그림을 하나로 연결하면 다음과 같다. 

#### [NN]
![image](https://user-images.githubusercontent.com/57993534/126037875-9ea96a9a-6847-4337-bfd9-2c932c5513ea.png)

: logistic regression을 multinomial classification으로 변환할 때 행렬을 사용하여 처리했음. 오른쪽은 왼쪽 그림의 첫번째 logistic regression들을 하나로 결합할 수 있음을 보여줌.    
- 오른쪽 그림에서 k의 계산은 다음과 같음     
 ![image](https://user-images.githubusercontent.com/57993534/126037885-15d65f48-39af-4f25-bac9-6b02f6e337de.png)    
- Y(y hat)의 계산은 다음과 같음    
![image](https://user-images.githubusercontent.com/57993534/126037892-9819fd76-b249-493b-8a29-5f62cf32a092.png)    

#### [Backpropagation]
![image](https://user-images.githubusercontent.com/57993534/126037899-a3c60cdc-b5da-460f-aece-e8af6fca5859.png)

- 앞에서부터 뒤로 진행하면서 W와 b를 바꾸어 나갈 때 : forward propagation
- 뒤에서부터 앞으로 거꾸로 진행하면서 바꾸어 나갈 때 : backward propagation
- NN에 포함된 여러 layer 중에서 결과에 가장 큰 영향을 주는 것은 뒤쪽 layer이므로 많은 layer 중에서 뒤쪽에 있는 일부만 수정하는 것이 훨씬 더 좋은 예측을 할 수 있게 함

![image](https://user-images.githubusercontent.com/57993534/126037903-a926d539-e4b9-4e8b-9e5a-83a2915e1ca4.png)     
![image](https://user-images.githubusercontent.com/57993534/126037906-c58c8a8c-7226-4978-93b5-08c8887710d3.png)      

- chain rule을 이용하여 길게 늘어져 있는 계산을 간단한 미분을 통해 동작하는 것

- 단점     
 ![image](https://user-images.githubusercontent.com/57993534/126037911-9a01f38c-fd48-452a-925f-8e792088d949.png)
    
- Vanishing gradient : sigmoid는 전달된 값을 0과 1 사이로 심하게 변형되어 값이 현저하게 작아지는 현상이 발생
- layer를 지날 때마다 최초 값보다 현저하게 작아져서 값을 전달해도 의미가 있을 수 없음

### 10. Neural Network 2: ReLU and 초기값 정하기 (2006/2007 breakthrough)
#### [ReLU : Rectified Linear Unit]
![image](https://user-images.githubusercontent.com/57993534/126037924-d03b8c78-14a1-4ef3-b76c-dbeb346bf5d7.png)

- 그림에 있는 것처럼 0보다 작을 때에는 0을 사용하고 0보다 큰 값에 대해서는 해당 값을 그대로 사용하는 방법
- 0과 현재 값(X) 중에서 큰 값을 선택. max(0, X)

#### [Activation Functions]
![image](https://user-images.githubusercontent.com/57993534/126037929-5c96f4b6-6d07-408b-a72a-2008afee3259.png)

1. tanh : sigmoid 함수를 재활용하기 위한 함수. sigmoid의 범위를 -1에서 1로 넓힘    
2. ReLU : max(0, X). 음수에 대해서만 0으로 처리    
3. Leaky ReLU : ReLU 함수의 변형으로 음수에 대해 1/10로 값을 줄여서 사용    
4. Maxout : 두 개의 W와 b 중에서 큰 값이 나온 것을 사용    
5. ELU : ReLU를 0이 아닌 다른 값을 기준으로 사용    

#### [Weight의 초기화]
- set all initial weights to 0 : Deep Learning Algorithm은 동작하지 않음.
- RBM : 현재 layer에 들어온 input x값에 대해 weight를 계산한 값을 다음 layer에 전달(forward). 전달받은 값을 거꾸로 이전(현재) layer에 대해 weight 값을 계산(backward)
  + forward와 backward 계산을 반복해서 진행하면서 input x 와 전달받아 다시 계산된 값(x hat)의 차가 최소가 되도록 weight를 조절
- Xavier / He
  + Xavier : 입력값(fan-in)과 출력값(fan-out) 사이의 난수를 선택해서 입력값(fan-in)의 제곱근으로 나눔
  + He : 입력값(fan-in)을 반으로 나눈 제곱근 사용. 분모가 작아지므로 Xavier보다 넓은 범위의 난수 생성 


#### [Oberfitting을 방지하는 방법]
1. training data 수 늘리기    
 : 데이터가 많으면 training set, validation set, test set으로 나누어 진행할 수 있고 영역별 데이터의 크기도 커지기 때문에 overfitting 확률이 낮아짐.    
2. 입력으로 들어오는 변수의 개수 줄이기    
 : 서로 비중이 다른 feature가 섞여 weight에 대해 경합을 하면 오히려 좋지 않은 결과가 나올 수도 있음.    
3. Regularization 사용    

#### [Regularization]
1. L2 regularization     
![image](https://user-images.githubusercontent.com/57993534/126037948-fb010903-5bd4-4270-9a08-663db0685406.png)

2. Dropout    
![image](https://user-images.githubusercontent.com/57993534/126037955-4f840d32-0be3-4b38-ac34-aeafdced6a91.png)      
    : 일부를 비활성화시킨 후 학습
3. Ensemble     
 : 데이터를 여러 개의 training set으로 나누어서 동시에 학습을 진행하여 모든 training set에 대한 학습이 끝나면 결과를 통합하는 방법     

### 11. Convolutional Neural Networks 
#### [CNN introduction]
![image](https://user-images.githubusercontent.com/57993534/126037983-794d052a-1c83-4687-82d2-5a8fbac33c0b.png)    

 : 위 그림에서 보는 것과 같이 한 개의 이미지에서 일부를 읽으면서 전체를 읽는 방식을 CNN이라고 할 수 있음.    
- 작은 필터가 차지하는 영역으로부터 읽어온 값을 공식을 통해 하나의 값으로 변환
- 동일한 필터를 사용하여 계속해서 다른 영역도 조사
- 같은 크기의 필터를 수평과 수직으로 이동하며 조사
- stride : 필터가 이동하는 크기
- 출력 크기 = 1 + (입력 크기 - 필터 크기) / stride 크기 (나누어떨어져야 함)
- padding : convolutional layer를 거치면서 크기가 작아져서 원본 이미지 크기를 유지하기 위해 사용

#### [Pooling layer(sampling)]
- Max Pooling : 여러 개의 값 중에서 가장 큰 값을 꺼내서 모아 놓는 것

#### [Fully Connected layer(FC layer)]
: softmax xlassifier와 연결하여 데이터를 labeling

#### [CNN case]
1. LeNet-5    
 ![image](https://user-images.githubusercontent.com/57993534/126037988-e6860087-dc6a-4d0c-b0a7-f359b53fb035.png)

2. AlexNet    
 ![image](https://user-images.githubusercontent.com/57993534/126037989-a0b012d6-8f29-4a19-80e7-60787de072e9.png)    

 : ReLU, dropout, ensemble을 적용하여 좋은 성능을 가지고 있음

3. GoogLeNet      
 ![image](https://user-images.githubusercontent.com/57993534/126037994-e96d00bc-6717-44db-b12c-3b6510e19746.png)

4. ResNet     
![image](https://user-images.githubusercontent.com/57993534/126037998-010fb846-7357-45e1-8ed8-6e4d6f112c45.png)

### 12. Recurrent Neural Network
#### [Sequence data]
- RNN은 Sequence data를 처리하는 모델
- 음성인식, 자연어처리 등이 포함      
![image](https://user-images.githubusercontent.com/57993534/126037999-ba8afa2f-a1c8-4893-932e-24c7bfd6a53e.png)

#### [Recurrent Neural Network]
![image](https://user-images.githubusercontent.com/57993534/126038006-d5a39edb-7c99-4b21-9022-0d6111d01f85.png)

- y = Wx + b     
![image](https://user-images.githubusercontent.com/57993534/126038010-efd7e64d-e5ac-47e2-b287-2cf925900e16.png)     
: 이전 단계에서의 상태 값과 입력 벡터(x)로 계산하면 새로운 상태의 값

#### [(Vanilla) Recurrent Neural Network]
![image](https://user-images.githubusercontent.com/57993534/126038037-6e8579f0-9c37-4228-b8c4-8dc14bd1c3c6.png)

: 가장 단순한 형태의 RNN 모델    
- t : sequence data에 대한 특정 시점
- W의 이전 상태와 입력을 가지고 fw에 해당하는 tanh 함수를 호출
- ht : 현재 상태
- yt : 예측값

#### [Character-level language model ex]
![image](https://user-images.githubusercontent.com/57993534/126038042-e27f5d9a-f8fc-41d3-8640-6074b835f219.png)

- hello가 들어왔다면 hell에 대한 예측은 ello가 됨
- 위의 사진에서는 첫 번째 예측은 틀렸음. 네 번째가 가장 크기 때문에 one-hot encoding을 거치면 o를 예측함.
#### [Recurrent Networks의 여러 가지 형태]
![image](https://user-images.githubusercontent.com/57993534/126038045-ea8e4698-6985-4e57-861d-165f426a7f48.png)

- one to one : Vanilla Neural Networks
- one to many : Image Captioning (한 장의 그림에 대해 여러 개의 단어 형태로 표현될 수 있음)
- many to one : Sentiment Classification (여러 개의 입력에 대해 하나의 결과)
- many to many : Machine Translation (기계번역)
- many to many : video classification on frame level (다대다 모델의 또 다른 형태로 동영상 같은 경우 여러 개의 이미지 프레임에 대해 여러 개의 설명이나 번역 형태로 결과를 반환)

[Several advanced models]
- Long Short Term Memory(LSTM)
- GRU

## 종합
- Linear Regression     
![image](https://user-images.githubusercontent.com/57993534/126038067-c87cd4dc-8e2a-4c15-b11b-a9c0dc9bd7b1.png)
- Logistic(Regression) classification       
![image](https://user-images.githubusercontent.com/57993534/126038070-0a6fe9a2-71a3-49d4-88a1-debb7d922e56.png)
- softmax classification      
![image](https://user-images.githubusercontent.com/57993534/126038086-759a27a4-0cb9-450c-9b80-9e89486209c2.png)
- 딥러닝 기본        
![image](https://user-images.githubusercontent.com/57993534/126038100-9e3345d4-cc02-4260-99d4-f8657334a880.png)
- ReLU         
![image](https://user-images.githubusercontent.com/57993534/126038108-169d0d2c-9d56-4504-bc5a-675e03e663b1.png)
- CNN    
![image](https://user-images.githubusercontent.com/57993534/126038114-35a71ba3-1c4f-4abe-88ce-984c3798b0c0.png)
- RNN      
![image](https://user-images.githubusercontent.com/57993534/126038286-09c129c7-12c3-41b9-adae-840580f1637d.png)
