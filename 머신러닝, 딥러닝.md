## 김성훈 교수님의 모두를 위한 머신러닝/딥러닝 강의 시즌 1 – 딥러닝의 기본 총정리 (https://hunkim.github.io/ml/)
### 1. 머신러닝의 개념과 용어
#### [Supervised / Unsupervised Learning]
- Supervised Learning : labeled data를 가지고 학습
- UnSupervised Learning : un-labeled data를 가지고 학습
#### [Types of supervised learning]
- regression : final exam score based on time spent (0 ~ 100)
- binary classification : pass / non-pass
- multi-label classification : Letter grade (A, B, C, D and F)

#### [Regression]
|x|y|
|------|---|
|10|90|
|9|80|
|3|50|
|2|30|

↓

(training) 

↓

Regression

- 소요된 시간을 기준으로 한 시험점수 예측

#### [Binary classification]
|x|y|
|------|---|
|10|P|
|9|P|
|3|F|
|2|F|

↓

(training) 

↓

Binary classification

- 소요된 시간을 기준으로 한 합격 / 불합격

#### [Multi-label Classification]
|x|y|
|------|---|
|10|A|
|9|B|
|3|D|
|2|F|

↓

(training) 

↓

Multi-class classification

- 소요된 시간을 기준으로 A, B, C, D, F 등 등급으로 평가

### 2. Linear Regression의 개념 
#### [Regression (data)]

![image](https://user-images.githubusercontent.com/57993534/126035167-7232c721-3d8f-44a1-a337-6bc87215c092.png)

#### [(Linear) Hypothesis]

![image](https://user-images.githubusercontent.com/57993534/126035170-d73c28fa-aee8-40d1-b40c-594023d41c0f.png)

- W와 b에 따라 선이 나타남.

![image](https://user-images.githubusercontent.com/57993534/126035173-12776cd2-00ca-4d5d-bf7f-9a82fd4cf998.png)

- 실제 데이터와의 거리를 계산하여 좋은 가설인지를 판단 (cost function)

#### [Cost function]
- H(x) - y = 가설 값 - 실제 값
- (H(x) - y) ^2 : 차이가 음수 / 양수 상관없음
- 
-  : m은 데이터 개수
-  : 이 값을 가장 작게 하는 W, b 값을 찾아야 함 -> 이것을 찾는 것이 Linear Regression의 학습
3. Linear Regression cost 함수 최소화
#### [Simplified hypothesis]


 : b = 0을 대입하여 단순화시킨다. 


- 아래의 값들을 이용하면 위 그래프처럼 표현된다.
    - W = 0일 때 cost(W) = 4.67 
    - W = 1일 때 cost(W) = 0
    - W = 2일 때 cost(W) = 4.67
- 결국, 목표는 cost 값이 최솟값이 되는 지점을 찾는 것

[Gradient descent algorithm]
- 경사를 따라 내려가는 알고리즘
- Minimize cost function : cost(W, b) - 최소의 W, b를 찾는 알고리즘
- 많은 minimize problems에 사용

[Gradient descent algorithm 작동방법]
1. 0.0(혹은 다른 값)에서 시작하여 W와 b를 조금씩 변경하여 cost(W, b)를 감소
2. 파라미터를 변경할 때마다 cost(W, b)를 가장 많이 감소시키는 gradient를 선택
3. 최저점으로 수렴할 때까지 반복
(시작할 때 최종 최솟값을 설정할 수 있음)

[Gradient descent algorithm 계산 방식]
 → 




- 이 식을 기계적으로 적용하면 cost function을 최소화하는 W를 구할 수 있고, 그것이 바로 Linear regression의 핵심인 학습 과정을 통해 모델을 만드는 과정이다.



4. 여러 개의 입력(feature)의 Linear Regression
[3개의 input(x1, x2, x3)을 사용하는 Regression]
- 일반적인 가설과 cost 함수 
    →  
  
: 변수가 많아지면 같은 방식으로 늘려줌. 하지만 이 방식을 사용하면 번거로워지므로 Matrix(행렬)를 사용함.

- Matrix(행렬)의 곱을 이용한 가설

: H(X) = XW로 표현할 수 있음. (여기서 X와 W는 모두 행렬)

: 각각의 인스턴스를 계산할 필요 없이 전체를 긴 행렬에 넣어 W를 곱하면 됨. (W 행렬의 크기는 변수의 개수가 m개이고 라벨의 개수가 n개이면 [m, n])
: Matrix를 사용하면 tensorflow의 shape의 instance의 수를 자동으로 채워줌. 즉, w = variable shape를 몰라도 자동으로 shape를 선정해줌.


5. Logistic(Regression) Classification 
[Binary Classification]
- 소요된 시간을 기준으로 두 개 중 하나의 결과값을 가짐. (ex> P/F, 0/1)

[공부한 시간을 기준으로 Pass(1) or Fail(0)을 예측]
- Linear Regression을 통해 실제 데이터와 가장 근접한 선형적 모델을 만들고, 그 중간 지점을 찾아 그 지점보다 작으면 Fail(0), 크면 Pass(1)로 예측

: 이렇게 하면 training set의 값이 커지면 중간 지점이 달라지고, 그렇게 되면 이전에는 Pass(1)였던 시간도 Fail(0)로 예측될 수 있음.

[Logistic Classification의 가설(Hypothesis) 정의]
- Logistic Hypothesis
: H(X) = WX를 z라고 하면 z 값에 상관없이 0에서 1 사이의 값이 나올 수 있도록 해주는 함수
   → 

[Logistic Regression의 cost 함수]
: Linear Regression과 동일한 cost 함수를 사용하게 되면 cost 함수의 모양이 매끄럽지 못하므로 매끄럽게 바꿔주어야 함.



[cost 함수의 최소화(Gradient descent algorithm)]




6. Softmax Regression(Multinomial Logistic Regression) 
[Multinomial Classification]
   
(hours)
(attendance)
(grade)
10
5
A
9
5
A
3
2
B
2
4
B
11
1
C

⇒

: Label의 종류가 세 개(A, B, C)인 Regression
: 3개의 각각 다른 Binary Classification으로도 구현할 수 있음. (각각 독립된 Classifier들을 가지고 구현)

[Classifier의 구현]
- Binary Classification
A :  
B :  
C :  
: 각각 독립된 Classifier들을 가지고 있어야 함. 이러한 방식을 이용하면 복잡하므로 Multinomial Classification을 이용함.

[Multinomial Classification]

: 3번의 독립된 형태의 벡터를 계산하는 것이 아니라 Matrix multiplication을 이용하여 편리하게 계산
[Softmax 함수]

: softmax 함수를 이용하여 각각의 scores가 0에서 1 사이의 값을 가지고 합이 1인 확률(probabilities)로 변환

[One-hot encoding]

: 제일 큰 값을 1로 만들고 나머지는 0으로 만든다.

[Cross-entropy Cost function]
- 하나의 training set에 대한 Cross-entropy cost function

ex) 
    ⇒ ⊙⊙ ⇒ 0
    ⇒ ⊙⊙ ⇒ 

- 여러 개의 training set에 대한 Cross-entropy cost function


[Cost function의 최소화(Gradient descent)]



7. ML의 실용과 몇 가지 팁
[Learning rate]
1. Large learning rate : overshooting

: learning rate가 너무 크면 경사면을 타고 튕겨 올라가다가 결국에는 밥그릇을 탈출하는 overshooting 현상이 발생함. 
2. Small learning rate

: learning rate가 너무 작은 경우에는 한참을 내려갔음에도 불구하고 최저점까지 도달하지 못하는 현상이 발생함.
- learning rate를 너무 크지도, 작지도 않게 조절하는 것이 중요
- 다양한 learning rate를 사용하여 여러 번에 걸쳐 실행하는 것이 최선

[Data(X) preprocessing for gradient descent]
- gradient descent algorithm을 적용하기 전에 preprocessing 작업으로 데이터의 범위를 제한할 수 있음.

[Feature Scaling]
1. Normalization(Re-Scaling)
 : 전체 구간을 0 ~ 100으로 설정하여 데이터를 관찰하는 방법으로 특정 데이터의 위치를 확인할 수 있도록 함
2. Standardization
 : 평균까지의 거리로 2개 이상의 대상이 단위가 다를 때 대상 데이터를 같은 기준으로 볼 수 있도록 함

[Overfitting]
- 모델이 training dataset에 잘 맞추기 위해 과도하게 복잡하여 실제로 사용할 때 맞지 않는 상황이 발생
- 해결책
    1. training data 수 늘리기
    2. 입력으로 들어오는 변수의 개수 줄이기
    3. Regularization 사용

[Regularization]
- W(weight)가 너무 큰 값들을 갖지 않도록 하는 것
- 모델의 복잡도를 낮추는 방법
  
- cost 함수가 틀렸을 때 높은 비용이 발생할 수 있도록 penalty를 부과하는 것처럼 W에 대한 값이 크면 penalty 부여






[Training, Validation and test sets]

: 위 그림처럼 학습을 시키기 위한 training set, 학습 결과를 확인하기 위한 test set이 필요
: 여러 번에 걸쳐 학습(train)하고, 검증(test)하는 작업을 반복


8. 딥러닝의 기본 개념과 문제, 그리고 해결
[Logistic regression Units]

: Logistic Regression을 여러 번 적용하면 오른쪽과 같은 형태로 구성할 수 있음

[Backpropagation]

- Output layer부터 Input layer까지 반대로 에러를 보정하는 기술
- layer가 많으면 뒤에서부터 멀리 떨어진 layer의 W와 b를 변경할 수 없는 엄청난 문제점 발생

[Breakthrough]
- 여러 개의 layer가 있어도 초깃값을 잘 선택하면 학습 가능
- 신경망을 잘 구성하면 복잡한 문제를 효율적으로 풀 수 있음






9. Neural Network 1: XOR 문제와 학습방법, Backpropagation (1986 breakthrough) 
[XOR using NN]

- 3개의 logistic regression의 과정에서 마지막에는 sigmoid 과정이 있음
- sigmoid 과정으로 hypothesis의 결과는 항상 0과 1 사이의 값으로 조정됨
- 앞 두 개의 결과를 마지막 logistic regression의 입력으로 사용한다.

[Forward propagation]

: 이전의 그림을 하나로 연결하면 다음과 같다. 

[NN]

: logistic regression을 multinomial classification으로 변환할 때 행렬을 사용하여 처리했음. 오른쪽은 왼쪽 그림의 첫번째 logistic regression들을 하나로 결합할 수 있음을 보여줌.
- 오른쪽 그림에서 k의 계산은 다음과 같음
 

- Y(y hat)의 계산은 다음과 같음


[Backpropagation]

- 앞에서부터 뒤로 진행하면서 W와 b를 바꾸어 나갈 때 : forward propagation
- 뒤에서부터 앞으로 거꾸로 진행하면서 바꾸어 나갈 때 : backward propagation
- NN에 포함된 여러 layer 중에서 결과에 가장 큰 영향을 주는 것은 뒤쪽 layer이므로 많은 layer 중에서 뒤쪽에 있는 일부만 수정하는 것이 훨씬 더 좋은 예측을 할 수 있게 함


- chain rule을 이용하여 길게 늘어져 있는 계산을 간단한 미분을 통해 동작하는 것

- 단점
 
    
- Vanishing gradient : sigmoid는 전달된 값을 0과 1 사이로 심하게 변형되어 값이 현저하게 작아지는 현상이 발생
- layer를 지날 때마다 최초 값보다 현저하게 작아져서 값을 전달해도 의미가 있을 수 없음



10. Neural Network 2: ReLU and 초기값 정하기 (2006/2007 breakthrough)
[ReLU : Rectified Linear Unit]

- 그림에 있는 것처럼 0보다 작을 때에는 0을 사용하고 0보다 큰 값에 대해서는 해당 값을 그대로 사용하는 방법
- 0과 현재 값(X) 중에서 큰 값을 선택. max(0, X)

[Activation Functions]

1. tanh : sigmoid 함수를 재활용하기 위한 함수. sigmoid의 범위를 -1에서 1로 넓힘
2. ReLU : max(0, X). 음수에 대해서만 0으로 처리
3. Leaky ReLU : ReLU 함수의 변형으로 음수에 대해 1/10로 값을 줄여서 사용
4. Maxout : 두 개의 W와 b 중에서 큰 값이 나온 것을 사용
5. ELU : ReLU를 0이 아닌 다른 값을 기준으로 사용

[Weight의 초기화]
* set all initial weights to 0 : Deep Learning Algorithm은 동작하지 않음.
* RBM : 현재 layer에 들어온 input x값에 대해 weight를 계산한 값을 다음 layer에 전달(forward). 전달받은 값을 거꾸로 이전(현재) layer에 대해 weight 값을 계산(backward)
  - forward와 backward 계산을 반복해서 진행하면서 input x 와 전달받아 다시 계산된 값(x hat)의 차가 최소가 되도록 weight를 조절
* Xavier / He
  - Xavier : 입력값(fan-in)과 출력값(fan-out) 사이의 난수를 선택해서 입력값(fan-in)의 제곱근으로 나눔
  - He : 입력값(fan-in)을 반으로 나눈 제곱근 사용. 분모가 작아지므로 Xavier보다 넓은 범위의 난수 생성 


[Oberfitting을 방지하는 방법]
1. training data 수 늘리기
 : 데이터가 많으면 training set, validation set, test set으로 나누어 진행할 수 있고 영역별 데이터의 크기도 커지기 때문에 overfitting 확률이 낮아짐.
2. 입력으로 들어오는 변수의 개수 줄이기
 : 서로 비중이 다른 feature가 섞여 weight에 대해 경합을 하면 오히려 좋지 않은 결과가 나올 수도 있음.
3. Regularization 사용

[Regularization]
1. L2 regularization
  
2. Dropout

    : 일부를 비활성화시킨 후 학습
3. Ensemble
 : 데이터를 여러 개의 training set으로 나누어서 동시에 학습을 진행하여 모든 training set에 대한 학습이 끝나면 결과를 통합하는 방법

11. Convolutional Neural Networks 
[CNN introduction]

 : 위 그림에서 보는 것과 같이 한 개의 이미지에서 일부를 읽으면서 전체를 읽는 방식을 CNN이라고 할 수 있음.
- 작은 필터가 차지하는 영역으로부터 읽어온 값을 공식을 통해 하나의 값으로 변환
- 동일한 필터를 사용하여 계속해서 다른 영역도 조사
- 같은 크기의 필터를 수평과 수직으로 이동하며 조사
- stride : 필터가 이동하는 크기
- 출력 크기 = 1 + (입력 크기 - 필터 크기) / stride 크기 (나누어떨어져야 함)
- padding : convolutional layer를 거치면서 크기가 작아져서 원본 이미지 크기를 유지하기 위해 사용

[Pooling layer(sampling)]
- Max Pooling : 여러 개의 값 중에서 가장 큰 값을 꺼내서 모아 놓는 것

[Fully Connected layer(FC layer)]
: softmax xlassifier와 연결하여 데이터를 labeling

[CNN case]
1. LeNet-5
 

2. AlexNet
 
 : ReLU, dropout, ensemble을 적용하여 좋은 성능을 가지고 있음

3. GoogLeNet
 
    
4. ResNet

12. Recurrent Neural Network
[Sequence data]
- RNN은 Sequence data를 처리하는 모델
- 음성인식, 자연어처리 등이 포함


[Recurrent Neural Network]

- y = Wx + b

: 이전 단계에서의 상태 값과 입력 벡터(x)로 계산하면 새로운 상태의 값

[(Vanilla) Recurrent Neural Network]

: 가장 단순한 형태의 RNN 모델
- t : sequence data에 대한 특정 시점
- W의 이전 상태와 입력을 가지고 fw에 해당하는 tanh 함수를 호출
- ht : 현재 상태
- yt : 예측값

[Character-level language model ex]

- hello가 들어왔다면 hell에 대한 예측은 ello가 됨
- 위의 사진에서는 첫 번째 예측은 틀렸음. 네 번째가 가장 크기 때문에 one-hot encoding을 거치면 o를 예측함.
[Recurrent Networks의 여러 가지 형태]

- one to one : Vanilla Neural Networks
- one to many : Image Captioning (한 장의 그림에 대해 여러 개의 단어 형태로 표현될 수 있음)
- many to one : Sentiment Classification (여러 개의 입력에 대해 하나의 결과)
- many to many : Machine Translation (기계번역)
- many to many : video classification on frame level (다대다 모델의 또 다른 형태로 동영상 같은 경우 여러 개의 이미지 프레임에 대해 여러 개의 설명이나 번역 형태로 결과를 반환)

[Several advanced models]
- Long Short Term Memory(LSTM)
- GRU
